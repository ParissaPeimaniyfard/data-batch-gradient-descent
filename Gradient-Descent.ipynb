{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create the necessary functions to go through the steps of a single Gradient Descent Epoch. You will then combine the functions and create a loop through the entire Gradient Descent procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import for you the following dataset of ingredients with their mineral content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliment</th>\n",
       "      <th>zinc</th>\n",
       "      <th>phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Durum wheat pre-cooked. whole grain. cooked. u...</td>\n",
       "      <td>0.120907</td>\n",
       "      <td>0.193784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian noodles. plain. cooked. unsalted</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>0.060329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rice. brown. cooked. unsalted</td>\n",
       "      <td>0.156171</td>\n",
       "      <td>0.201097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rice. cooked. unsalted</td>\n",
       "      <td>0.065491</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rice. parboiled. cooked. unsalted</td>\n",
       "      <td>0.025189</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             aliment      zinc  phosphorus\n",
       "0  Durum wheat pre-cooked. whole grain. cooked. u...  0.120907    0.193784\n",
       "1             Asian noodles. plain. cooked. unsalted  0.047859    0.060329\n",
       "2                      Rice. brown. cooked. unsalted  0.156171    0.201097\n",
       "3                             Rice. cooked. unsalted  0.065491    0.045704\n",
       "4                  Rice. parboiled. cooked. unsalted  0.025189    0.045704"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/04-Under-the-Hood/gradient_descent_ingredients_zinc_phosphorous.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá We can visualize a somewhat Linear relationship between the `Phosphorus` and `Zinc`.   \n",
    "\n",
    "Let's use Gradient Descent to find the line of best fit between them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=data, x='zinc', y='phosphorus');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create the two `np.Array`\n",
    "- `data_X` for zinc\n",
    "- `data_Y` for phosphorus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "data_X= np.array(data['zinc'])\n",
    "data_Y= np.array(data['phosphorus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data_X.shape == (53,))\n",
    "assert (data_Y.shape == (53,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code one Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the exercise, you will define the key functions used to update the parameters during one epoch $\\color {red}{(k)}$ of gradient descent. Recall the formula below\n",
    "\n",
    "$$\n",
    "\\beta_0^{\\color {red}{(k+1)}} = \\beta_0^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_0}(\\beta^{\\color{red}{(k)}})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta_1^{\\color {red}{(k+1)}} = \\beta_1^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_1}(\\beta^{\\color {red}{(k)}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} =  a x + b\n",
    "$$\n",
    "\n",
    "‚ùì Define the hypothesis function of a Linear Regression. Let `a` be the slope and `b` the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X,a,b):\n",
    "    return a*X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Sum\\ Squares\\ Loss = \\sum_{i=0}^n (y^{(i)} - \\hat{y}^{(i)} )^2\n",
    "$$\n",
    "\n",
    "‚ùì Define the SSR Loss Function for the Hypothesis Function using the equation above. Reuse the function `h` coded above when writing your new function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X,Y,a,b):\n",
    "    y_pred= h(X,a,b)\n",
    "    loss_value= sum((Y-y_pred)**2)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What would be the total Loss computed on all our ingredients dataset if:\n",
    "- a = 1 \n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.86850698611546\n"
     ]
    }
   ],
   "source": [
    "print(loss(data_X, data_Y, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting 63.86. If not, something is wrong with your function. Fix it before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d\\ SSR}{d\\ slope}= \\sum_{i=0}^n -2  x_i (y^{(i)} - \\hat{y}^{(i)} )\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\ SSR}{d\\ intercept}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} ) \n",
    "$$\n",
    "\n",
    "‚ùì Define a function to compute the partial derivatives of the Loss Function relative to parameter `a` and `b` at a given point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Again, you must re-use the Hypothesis Function in your new function to compute the predictions at given points.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,a,b):\n",
    "    d_b= np.sum(-2*(Y- h(X,a,b)))\n",
    "    d_a= np.sum(-2*X*(Y- h(X,a,b)))\n",
    "    return d_a, d_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using your function, what would be the partial derivatives of each parameter if:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48.459065809109006, 115.17923733301406)\n"
     ]
    }
   ],
   "source": [
    "a=1\n",
    "b=1\n",
    "print(gradient(data_X, data_Y, a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.17923733301406"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(data_X, data_Y, a, b)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting 48.45 and  115.17. If not, fix your function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Step Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "step\\ size = gradient \\cdot learning\\ rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Define a function that calculates the step sizes alongside each parameter (`a`,`b`), according to their derivatives (`d_a`, `d_b`) and a `learning_rate` equal to `0.01` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps(d_a,d_b, learning_rate = 0.01):\n",
    "    step_a= d_a * learning_rate\n",
    "    step_b= d_b * learning_rate\n",
    "    return (step_a, step_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What would be the steps (`step_a`, `step_b`) to take for the derivatives computed above for (`a`,`b`) = (1,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4845906580910901, 1.1517923733301405)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps(gradient(data_X, data_Y, a, b)[0], gradient(data_X, data_Y, a, b)[1], learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è The steps should be 0.48 for `a` and 1.15 for `b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Update parameters (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "updated\\ parameter = old\\ parameter\\ value - step\\ size\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Define a function that computes the updated parameter values from the old parameter values and the step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(a, b, step_a, step_b):\n",
    "    a_new= a - step_a\n",
    "    b_new= b - step_b\n",
    "    return a_new , b_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 One full epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using the functions you just created, compute the updated parameters at the end of the first Epoch, had you started with parameters:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5154093419089099, -0.1517923733301405)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=1\n",
    "b=1\n",
    "updat_ab= update_params(a,b, steps(gradient(data_X, data_Y, a, b)[0], \n",
    "                         gradient(data_X, data_Y, a, b)[1], learning_rate = 0.01)[0],\n",
    "             steps(gradient(data_X, data_Y, a, b)[0], \n",
    "                   gradient(data_X, data_Y, a, b)[1], learning_rate = 0.01)[1])\n",
    "updat_ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting the following values:\n",
    "   - updated_a = 0.51\n",
    "   - updated_b = -0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now that you have the necessary functions for a Gradient Descent, loop through epochs until convergence.\n",
    "\n",
    "- Initialize parameters `a = 1` and  `b = 1`\n",
    "- Consider convergence to be **100 epochs**\n",
    "- Don't forget to start each new epoch with the updated parameters\n",
    "- Append the values for `loss`, `a`, and `b` at each epoch to their corresponding lists called `loss_history`, `a_history` and `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5154093419089099, 0.6475338480410271, 0.6273545046153022, 0.6437520254549167, 0.6504040813696262, 0.6587355634769616, 0.6659889132103662, 0.6728856930511895, 0.6792902431448742, 0.685274609433956, 0.6908572504746171, 0.6960673673055952, 0.7009292694951631, 0.7054663644254688, 0.7097003182996756, 0.7136513932777129, 0.7173384870685168, 0.7207792374040749, 0.7239901029529684, 0.7269864428989624, 0.7297825902031633, 0.7323919202167937, 0.7348269146495194, 0.7370992212786975, 0.7392197096656721, 0.7411985231504238, 0.7430451273721139, 0.7447683555479074, 0.7463764507265975, 0.7478771052191667, 0.7492774973948976, 0.7505843260190471, 0.7518038422963396, 0.7529418797735559, 0.7540038822442603, 0.7549949297891452, 0.7559197630765584, 0.7567828060394535, 0.7575881870372398, 0.7583397586037577, 0.7590411158758447, 0.7596956137906451, 0.7603063831339264, 0.7608763455161688, 0.7614082273480665, 0.7619045728822924, 0.7623677563839117, 0.7627999934876603, 0.7632033517964163, 0.7635797607715635, 0.7639310209625543, 0.7642588126198245, 0.7645647037322585, 0.764850157527652, 0.7651165394720509, 0.7653651238014482, 0.7655970996170813, 0.7658135765734894, 0.7660155901865369, 0.766204106786796, 0.7663800281419817, 0.7665441957705503, 0.7666973949670971, 0.7668403585588064, 0.7669737704109252, 0.7670982686980272, 0.7672144489567163, 0.7673228669343709, 0.7674240412475575, 0.767518455862829, 0.7676065624117752, 0.7676887823513987, 0.7677655089801517, 0.7678371093192757, 0.7679039258684451, 0.7679662782441115, 0.7680244647083867, 0.7680787635957775, 0.7681294346445963, 0.7681767202394172, 0.7682208465705213, 0.7682620247158758, 0.7683004516508238, 0.7683363111903149, 0.7683697748681816, 0.768401002757671, 0.7684301442371535, 0.7684573387046733, 0.7684827162447575, 0.7685063982506751, 0.7685284980051201, 0.7685491212220988, 0.768568366552612, 0.7685863260565506, 0.7686030856430646, 0.7686187254815076, 0.7686333203849272, 0.7686469401679322, 0.7686596499806505, 0.768671510620374, 0.7686825788223819] [-0.1517923733301405, 0.1253351689484678, 0.05199049289780815, 0.0650535505555884, 0.05723081117298024, 0.05484465067703296, 0.05141136896993313, 0.04850372390367696, 0.045717605226107995, 0.04313549419319603, 0.04072151480231099, 0.038469895918032446, 0.03636844869256201, 0.034407469100029356, 0.03257749130625461, 0.030869782620625882, 0.029276169076884233, 0.02778902890253442, 0.02640124806315708, 0.025106188196032494, 0.023897654373459715, 0.02276986558774631, 0.021717427068735737, 0.020735304485102753, 0.01981879986091536, 0.01896352910333335, 0.018165401031228108, 0.017420597805074742, 0.016725556664339815, 0.016076952885044953, 0.015471683875974845, 0.014906854337455947, 0.014379762411712762, 0.013887886758551618, 0.013428874494549345, 0.01300052993805334, 0.012600804106155236, 0.012227784913396666, 0.011879688025323446, 0.0115548483231349, 0.011251711938601302, 0.010968828821146717, 0.010704845801543406, 0.010458500119036969, 0.01022861338094014, 0.010014085925800232, 0.009813891563177196, 0.009627072664869157, 0.009452735584105615, 0.009290046380794758, 0.009138226832377291, 0.008996550711205158, 0.008864340310636704, 0.008740963203232555, 0.008625829215543406, 0.008518387605020177, 0.00841812442554106, 0.008324560068954125, 0.008237246970875663, 0.008155767469769027, 0.008079731809064263, 0.008008776272760062, 0.007942561445591538, 0.007880770589439532, 0.007823108128216495, 0.007769298233980325, 0.007719083507513788, 0.00767222374705694, 0.0076284947993042656, 0.007587687487168774, 0.007549606609185155, 0.007514070005764905, 0.007480907687837503, 0.007449961023708908, 0.0074210819802480125, 0.007394132414771414, 0.007368983414238909, 0.0073455146785988655, 0.007323613945333864, 0.007303176452453975, 0.007284104437368382, 0.007266306669238875, 0.007249698012577713, 0.007234199020002982, 0.007219735552202126, 0.007206238423287695, 0.007193643069846779, 0.007181889242102591, 0.007170920715710011, 0.007160685022807024, 0.007151133201034916, 0.007142219559327335, 0.007133901459347391, 0.0071261391115274445, 0.007118895384736097, 0.007112135628661722, 0.007105827508062814, 0.0070999408480927895, 0.007094447489958678, 0.00708932115622393, 0.0070845373251103025] [63.86850698611546, 4.956802678221286, 1.3935754675861143, 1.1686929441073393, 1.1463724193599885, 1.1373957745671934, 1.1302092310311778, 1.1239889297075834, 1.1185743402972523, 1.1138592422651652, 1.1097531588184455, 1.1061774212625193, 1.103063529178614, 1.1003518301665027, 1.097990376609746, 1.0959339304938414, 1.0941430967865544, 1.0925835687530439, 1.0912254707468423, 1.0900427858865411, 1.0890128576542648, 1.0881159558681286, 1.0873348987140807, 1.0866547235964512, 1.0860624005017252, 1.085546582384519, 1.0850973877939207, 1.0847062115760018, 1.0843655600261641, 1.084068907333361, 1.0838105705660996, 1.0835856008053721, 1.0833896883389507, 1.0832190801008683, 1.0830705077744964, 1.0829411251818863, 1.0828284537599662, 1.082730335079072, 1.0826448894942342, 1.0825704801371003, 1.082505681558687, 1.0824492524222709, 1.0824001117232829, 1.082357318080665, 1.082320051702973, 1.0822875986837555, 1.0822593373253533, 1.082234726229127, 1.0822132939239704, 1.0821946298344025, 1.0821783764152357, 1.0821642223021395, 1.0821518963468735, 1.0821411624229493, 1.0821318149021812, 1.0821236747155065, 1.0821165859225796, 1.0821104127244494, 1.082105036862078, 1.0821003553508697, 1.0820962785078119, 1.082092728233427, 1.0820896365156387, 1.0820869441268681, 1.0820845994894253, 1.0820825576874387, 1.0820807796064122, 1.0820792311839122, 1.0820778827570383, 1.0820767084941787, 1.082075685900154, 1.082074795385282, 1.0820740198901033, 1.0820733445585713, 1.08207275645346, 1.0820722443085282, 1.0820717983126906, 1.0820714099220723, 1.0820710716963318, 1.082070777156128, 1.082070520658999, 1.082070297291267, 1.0820701027739104, 1.082069933380588, 1.082069785866258, 1.0820696574050104, 1.0820695455359313, 1.0820694481159614, 1.082069363278842, 1.0820692893993602, 1.0820692250622181, 1.082069169034919, 1.082069120244155, 1.0820690777552497, 1.082069040754249, 1.0820690085323301, 1.0820689804722234, 1.0820689560363848, 1.0820689347567007, 1.0820689162255201, 1.0820689000878432]\n"
     ]
    }
   ],
   "source": [
    "a=1\n",
    "b=1\n",
    "n= data_X.shape[0]\n",
    "epochs= 101\n",
    "a_history= []\n",
    "b_history=[]\n",
    "loss_history=[]\n",
    "for epoch in range(epochs):\n",
    "    updated_ab=update_params(a, b, steps(gradient(data_X, data_Y, a, b)[0], \n",
    "                         gradient(data_X, data_Y, a, b)[1], learning_rate = 0.01)[0],\n",
    "             steps(gradient(data_X, data_Y, a, b)[0], \n",
    "                   gradient(data_X, data_Y, a, b)[1], learning_rate = 0.01)[1])\n",
    "    updated_a= updated_ab[0]\n",
    "    updated_b= updated_ab[1]\n",
    "    a_history.append(updated_a)\n",
    "    b_history.append(updated_b)\n",
    "    loss_his= loss(data_X, data_Y, a, b)\n",
    "    loss_history.append(loss_his)\n",
    "    a= updated_a\n",
    "    b= updated_b\n",
    "    \n",
    "print(a_history,  b_history, loss_history)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What are the parameter values at the end of the 100 epochs? Save them to respective variables `a_100` and `b_100` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "a_100= a_history[100]\n",
    "b_100= b_history[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/parissa/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/parissa/code/ParissaPeimaniyfard/05-ML/04-Under-the-hood/data-batch-gradient-descent/tests\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2, asyncio-0.19.0\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_descent.py::TestDescent::test_a \u001b[31mFAILED\u001b[0m\u001b[31m                              [ 50%]\u001b[0m\n",
      "test_descent.py::TestDescent::test_b \u001b[31mFAILED\u001b[0m\u001b[31m                              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ TestDescent.test_a ______________________________\u001b[0m\n",
      "\n",
      "self = <tests.test_descent.TestDescent testMethod=test_a>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_a\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        res = \u001b[96mself\u001b[39;49;00m.result.a_100\n",
      "        \u001b[96mself\u001b[39;49;00m.assertGreater(res, \u001b[94m0.74\u001b[39;49;00m)\n",
      ">       \u001b[96mself\u001b[39;49;00m.assertLess(res, \u001b[94m0.78\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       AssertionError: 1.0078562009803234 not less than 0.78\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_descent.py\u001b[0m:8: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ TestDescent.test_b ______________________________\u001b[0m\n",
      "\n",
      "self = <tests.test_descent.TestDescent testMethod=test_b>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_b\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        res = \u001b[96mself\u001b[39;49;00m.result.b_100\n",
      ">       \u001b[96mself\u001b[39;49;00m.assertGreater(res, \u001b[94m0.006\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       AssertionError: -0.1517923733301405 not greater than 0.006\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_descent.py\u001b[0m:12: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_descent.py::TestDescent::test_a - AssertionError: 1.0078562009803...\n",
      "FAILED test_descent.py::TestDescent::test_b - AssertionError: -0.151792373330...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m2 failed\u001b[0m\u001b[31m in 0.26s\u001b[0m\u001b[31m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/descent.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed descent step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üß™ Test your code\n",
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('descent',\n",
    "                         a_100=a_100,\n",
    "                         b_100=b_100)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Wrap this iterative approach into a method `gradient_descent()` which returns your `new_a`, `new_b` and `history`, a dictionary containing these lists: \n",
    "- `loss_history`\n",
    "- `a_history`\n",
    "- `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, a_init=1, b_init=1, learning_rate=0.001, n_epochs=100):\n",
    "    pass  # YOUR CODE HERE\n",
    "    return a, b, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the line of best fit through Zinc and Phosphorus using the parameters of your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize your descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Our goal is to plot our loss function and the descent steps on a 2D surface using matplotlib [contourf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Start by creating the data we need for the plot\n",
    "- `range_a`: a range of 100 values for `a` equally spaced between -1 and 1\n",
    "- `range_b`: a range of 100 values for `b` equally spaced between -1 and 1 \n",
    "- `Z`: a 2D-array where each element `Z[j,i]` is equal to the value of the loss function at `a` = `range_a[i]` and `b` = `range_b[j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now, plot in one single subplot:\n",
    "- your gradient as a 2D-surface using matplotlib [contourf](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contourf.html) with 3 parameters\n",
    "- all historical (a,b) points as a scatterplot with red dots to visualize your gradient descent!\n",
    "\n",
    "Change your learning rate and observe its impact on the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì [optional] What about 3D? Try to plot the same data on a [plot.ly 3D contour plot](https://plotly.com/python/3d-surface-plots/) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "surface = go.Surface(x=range_a, y=range_b, z=Z)\n",
    "scatter = go.Scatter3d(x=history['a'], y=history['b'], z=history['loss'], mode='markers')\n",
    "fig = go.Figure(data=[surface, scatter])\n",
    "\n",
    "#fig.update_layout(title='Loss Function', autosize=False, width=500, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the history of the `loss` values as a function of the number of `epochs`. Try with multiple variations of `learning_rate` from 0.001 to 0.01 and make sure to understand the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. With Sklearn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using Sklearn, train a Linear Regression model on the same data. Compare its parameters to the ones computed by your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They should be almost identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†üèÅ Congratulation! Please, push your exercise when you are done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
